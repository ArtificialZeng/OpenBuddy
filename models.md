# Model Downloads

### ModelScope 中文社区

欢迎访问我们的 ModelScope 中文社区，体验一部分热门模型的国内高速下载和一键部署！

https://modelscope.cn/organization/OpenBuddy

### OpenBuddy-OpenLLaMA-3B

- Tiny model, good for CPU/Mobile inference
- GGML format (5-bit, CPU/GPU, llama.cpp): https://huggingface.co/OpenBuddy/openbuddy-ggml
- Huggingface: https://huggingface.co/OpenBuddy/openbuddy-openllama-3b-v10-bf16

### OpenBuddy-Atom-13B

- Optimized for Chinese writing/reading tasks
- Math performance is not as good as other models, for now
- GGML format (5-bit, CPU/GPU, llama.cpp): https://huggingface.co/OpenBuddy/openbuddy-ggml
- Huggingface format (High-end GPU, 32GB+ VRAM required): https://huggingface.co/OpenBuddy/openbuddy-atom-13b-v9-bf16

### OpenBuddy-Falcon-40B

- Similar capabilities to LLaMA-30B/65B, with Apache 2.0 license
- Dataset is more diverse than LLaMA
- GGML format (3-bit, CPU/GPU, [ggllm.cpp](https://github.com/cmp-nct/ggllm.cpp) ): https://huggingface.co/OpenBuddy/openbuddy-ggml
- Huggingface format (High-end GPU, 96GB+ VRAM required): https://huggingface.co/OpenBuddy/openbuddy-falcon-40b-v9-bf16

### OpenBuddy-LLaMA2-13B

- Recommended for most use cases!
- Offers improved performance and capabilities over LLaMA1
- GGML format (3-bit, CPU/GPU, llama.cpp): https://huggingface.co/OpenBuddy/openbuddy-ggml
- Huggingface format (High-end GPU, 32GB+ VRAM required): https://huggingface.co/OpenBuddy/openbuddy-llama2-13b-v8.1-fp16

### OpenBuddy-LLaMA-30B

- Outstanding performance and capabilities
- Beats ChatGPT-3.5 in double-blind tests
- GGML format (3-bit, CPU/GPU, llama.cpp): https://huggingface.co/OpenBuddy/openbuddy-30b-ggml
- Huggingface format (High-end GPU, 80GB+ VRAM required) https://huggingface.co/OpenBuddy/openbuddy-llama-30b-v7.1-bf16


### OpenBuddy-Falcon-7B

- Huggingface format: https://huggingface.co/OpenBuddy/openbuddy-falcon-7b-v5-fp16

### OpenBuddy-LLaMA-7B

- GGML format (3-bit, CPU/GPU, llama.cpp): https://huggingface.co/OpenBuddy/openbuddy-llama-ggml
- Huggingface format: https://huggingface.co/OpenBuddy/openbuddy-llama-7b-v4-fp16


### OpenBuddy-OpenLLaMA-7B

- Limited capabilities, falcon-7b is recommended instead
- GGML format (3/5-bit, CPU/GPU, llama.cpp): https://huggingface.co/OpenBuddy/openbuddy-ggml
- Huggingface format: https://huggingface.co/OpenBuddy/openbuddy-openllama-7b-v5-fp16